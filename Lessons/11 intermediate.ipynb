{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf748873",
   "metadata": {},
   "source": [
    "# 11. The `BeautifulSoup` Library: Parsing Digital Artifacts (HTML)\n",
    "\n",
    "Often, the data we need to explore is located on the web, structured within HTML or XML documents. `BeautifulSoup` is a powerful Python library that allows us to parse these static documents, navigate their structure, and extract the valuable information within.\n",
    "\n",
    "- `BeautifulSoup` is ideal for parsing static HTML and XML files (the kind whose content doesn't change without a page reload).\n",
    "- For interacting with dynamic websites that rely heavily on JavaScript to load content, a browser automation tool like `Selenium` is often a better choice.\n",
    "- **Installation** (in your active virtual environment): `pip install beautifulsoup4`\n",
    "- **Documentation:** https://beautiful-soup-4.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# We need to import the BeautifulSoup and requests library\n",
    "\n",
    "# The url of the page we want to scrape\n",
    "url = \"http://example.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# First, confirm our request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Create a 'Soup' object from the page's HTML text\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # 'html.parser' helps Python make sense of the HTML structure.\n",
    "    \n",
    "    # Example: Extract the text of all product titles (which are inside <h3><a> tags on this site)\n",
    "    book_title_tags = soup.find_all('h3') # Find all <h3> tags\n",
    "\n",
    "    for title_tag in book_title_tags:\n",
    "        # The \".text\" attribute extracts text content from within \"a\" tag\n",
    "        print(title_tag.a.text) # Access the <a> tag inside the <h3> and get its text\n",
    "else:\n",
    "    print(f\"Failed to download the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "# --- Finding individual tags ---\n",
    "# Accessing tags directly gets the *first* occurrence on the page.\n",
    "first_title_tag = soup.title # The complete <title>...</title> tag\n",
    "title_name = soup.title.name # The name of the tag -> 'title'\n",
    "title_text = soup.title.string # The text content inside the tag\n",
    "\n",
    "first_h3_tag = soup.h3 # The first <h3> tag\n",
    "first_link = soup.a # The first <a> tag\n",
    "\n",
    "\n",
    "# --- Getting tag attributes and text ---\n",
    "# returns the value of an attribute from a tag.\n",
    "link_url = first_link.get('href') # -> 'index.html'\n",
    "# returns the inner text of a tag, similar to .string or .text\n",
    "link_text = first_link.get_text() # -> 'Home'\n",
    "\n",
    "\n",
    "# --- Using find_all() ---\n",
    "# returns a list-like ResultSet of all <a> tag elements.\n",
    "all_links = soup.find_all('a')\n",
    "\n",
    "for link in all_links: # We can iterate through the results\n",
    "    print(f\"Link Text: {link.get_text().strip()}, URL: {link.get('href')}\")\n",
    "\n",
    "\n",
    "# --- Using find() with filters ---\n",
    "# .find() is like .find_all() but returns only the first match.\n",
    "# We can also filter by attributes like 'class'. Note: 'class' is a Python keyword,\n",
    "# so we use 'class_' with a trailing underscore.\n",
    "article_pod = soup.find('div', class_='product_pod') # Example find\n",
    "print(article_pod)\n",
    "\n",
    "\n",
    "# --- Using CSS Selectors with select() ---\n",
    "# .select() and .select_one() use CSS selector syntax to find elements, which is very powerful.\n",
    "soup.select_one(selector=\"p a\") # Returns the first <a> tag that is inside a <p> tag\n",
    "soup.select(selector=\"p a\") # Returns a list of all <a> tags inside <p> tags\n",
    "soup.select(selector=\".product_pod\") # Returns a list of all tags with class=\"product_pod\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f3d4f",
   "metadata": {},
   "source": [
    "## 11.1. The `robots.txt` Protocol: Rules of Engagement\n",
    "- The `robots.txt` file is a standard text file located in the root directory of a website (e.g., `https://www.google.com/robots.txt`).\n",
    "- It provides rules and directives for automated programs (`bots` or `spiders`) that visit the site. It outlines which parts of the site the owner does *not* want bots to access.\n",
    "- It's like the \"rules of engagement\" or a set of access permissions left by the site's creators for automated reconnaissance bots.\n",
    "\n",
    "    - `User-agent: *` (* means the rule applies to all bots)\n",
    "    - `User-agent: Googlebot` (The rule applies to a specific bot, Google's main crawler)\n",
    "    - `\"Disallow:\"` (with no value) = Bots are allowed everywhere.\n",
    "    - `\"Disallow: /\"` = Bots are not allowed anywhere on the site.\n",
    "    - `\"Disallow: /search\"` = Bots should not access anything in the `/search/` directory.\n",
    "    - `\"Allow: /search/about\"` = An exception, allowing access to a specific sub-page even if its parent is disallowed.\n",
    "    - `\"Disallow: /index.html?\"` = Disallows access to any URL starting with `/index.html` that includes a query string (e.g., `/index.html?id=123`). The `?` denotes the start of a query string.\n",
    "\n",
    "- **Note:** Respecting `robots.txt` is a matter of etiquette and good practice. It is **not technically enforceable**; a malicious bot can simply ignore it. Responsible programmers and systems, however, adhere to these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474955b",
   "metadata": {},
   "source": [
    "## practice\n",
    "\n",
    "**Scenario:** You are a data operative tasked with extracting specific intel from publicly available web sources (static web pages).\n",
    "\n",
    "**1. Basic Reconnaissance:**\n",
    "- Go to a simple, static web page. \n",
    "- Inspect its structure in your browser using DevTools (`F12`) to identify the HTML tags that contain the data you want.\n",
    "- Using `requests` and `BeautifulSoup` in a Python script:\n",
    "    - **a)** Extract all visible text from the main body of the page and print it to the console.\n",
    "    - **b)** Extract all links, headlines or anything else from the page and print them as a list.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Challenge I: Modular Scraping Tool**\n",
    "- Refactor your code from the previous exercise into one or more functions.\n",
    "- Create a main function that accepts a `url` as a parameter to make your scraper reusable for different targets.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Challenge II: Data Archiving**\n",
    "- Modify your function(s) to save the extracted results to a local file.\n",
    "- If the file doesn't exist, it should be created. If it does exist, the new data should be appended.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Challenge III: Timestamped Logging**\n",
    "- Enhance your function(s) from the previous challenge.\n",
    "- Each time you run a scrape on a URL, the new results should be appended to the existing log file.\n",
    "- Before writing the new results for a scrape session, your script should first write a header line with the **current date and time** to timestamp when that specific data was gathered. This allows your log file to store a history of multiple reconnaissance runs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc2c59",
   "metadata": {},
   "source": [
    "---\n",
    "#### © Jiří Svoboda (George Freedom)\n",
    "- Web: https://GeorgeFreedom.com\n",
    "- LinkedIn: https://www.linkedin.com/in/georgefreedom/\n",
    "- Book me: https://cal.com/georgefreedom"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
